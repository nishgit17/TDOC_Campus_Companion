# ğŸ“ Campus Companion

**AI-Powered Campus Information Assistant for NIT Durgapur**

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Backend API** | FastAPI + Uvicorn |
| **Database** | SQLite3 |
| **Vector Storage** | ChromaDB (internally using SQLite3) |
| **Embeddings** | Sentence Transformers â†’ 384-dim vectors |
| **Frontend** | Streamlit |
| **PDF Loading** | PyPDF Loader |
| **Classification** | Keyword â†’ Logistic Regression (ML) â†’ LLM |
| **LLM** | Open Source Model from HuggingFace: Mistral-7B-Instruct |

---

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CAMPUS COMPANION SYSTEM                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  USER QUERY â†’ FastAPI Backend â†’ 3-Level Classification           â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  INTENT CLASSIFIER (core/classifier.py)                    â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  Level 1: Keyword Matching (âš¡ 0.001s) - 70% queries        â”‚  â”‚
â”‚  â”‚  Level 2: ML Classifier (âš¡âš¡ 0.01s) - 25% queries           â”‚  â”‚
â”‚  â”‚  Level 3: LLM (Mistral-7B) (âš¡âš¡âš¡ 1-2s) - 5% queries         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚         â–¼               â–¼               â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ DATABASE   â”‚  â”‚ RAG SYSTEM â”‚  â”‚ AI FALLBACKâ”‚                  â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚                  â”‚ 
â”‚  â”‚ â€¢ Canteen  â”‚  â”‚ â€¢ ChromaDB â”‚  â”‚ Mistral-7B â”‚                  â”‚
â”‚  â”‚ â€¢ Faculty  â”‚  â”‚ â€¢ 384-dim  â”‚  â”‚ Generates  â”‚                  â”‚
â”‚  â”‚ â€¢ Rooms    â”‚  â”‚   Vectors  â”‚  â”‚ Responses  â”‚                  â”‚
â”‚  â”‚ â€¢ Wardens  â”‚  â”‚ â€¢ Cosine   â”‚  â”‚            â”‚                  â”‚
â”‚  â”‚ (SQLite)   â”‚  â”‚   Search   â”‚  â”‚            â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚        â”‚               â”‚               â”‚                         â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                        â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  AI RESPONSE FORMATTER (core/response.py)                  â”‚  â”‚
â”‚  â”‚  Raw Data â†’ Natural Language (Mistral-7B, Temp: 0.5)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  JSON RESPONSE                                             â”‚  â”‚
â”‚  â”‚  {"answer": "...", "intent": "...", "confidence": 0.85}    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
CAMPUS_COMPANION/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                    # FastAPI app initialization
â”‚   â””â”€â”€ routers/
â”‚       â””â”€â”€ chat.py                # â­ Main chat endpoint (600+ lines)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ classifier.py              # â­ 3-level intent classification (800+ lines)
â”‚   â”œâ”€â”€ rag.py                     # â­ RAG system with ChromaDB (190+ lines)
â”‚   â”œâ”€â”€ response.py                # ğŸ¤– AI response formatter
â”‚   â”œâ”€â”€ fallback_message.py        # ğŸ›¡ï¸ AI fallback handler
â”‚   â””â”€â”€  embeddings.py              # Document chunking & embeddings
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ models.py                  # â­ Database schema (10 tables)
â”‚   â””â”€â”€ session.py                 # DB connection
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_pdfs.py             # PDF â†’ ChromaDB pipeline
â”‚   â”œâ”€â”€ pdf_processor.py           # Text extraction (PyPDF2 + Tesseract)
â”‚   â””â”€â”€ chunking.py                # Text chunking logic
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/                      # Source PDF documents
â”‚   â””â”€â”€ rag_docs/                  # ChromaDB storage
â”œâ”€â”€ frontend.py                    # Streamlit chat UI
â”œâ”€â”€ app.py                         # Database initializer
â”œâ”€â”€ testdb.py                      # Sample data loader
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env                           # Environment variables
â””â”€â”€ campus_companion.db            # SQLite database
```

---

## ğŸš€ Quick Start Guide

### ğŸ“‹ Prerequisites

Verify the following are installed:

```bash
# Verify installations
python3 --version    
pip --version
git --version
```

### ğŸ”§ Installation

**1. Clone and Navigate**
```bash
git clone <your-repo-url>
cd CAMPUS_COMPANION
```

**2. Create Virtual Environment**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate (Linux/Mac)
source .venv/bin/activate

# Activate (Windows)
.venv\Scripts\activate
```

**3. Install Dependencies**
```bash
pip install -r requirements.txt
```

**4. Set Up HuggingFace Token**
1. Visit: https://huggingface.co/settings/tokens
2. Create token (Read access)
3. Copy token
4. Create `.env` file:
```bash
echo "HUGGINGFACEHUB_API_TOKEN=hf_paste_your_token_here" > .env
```

**5. Initialize Database**
```bash
python3 app.py
python3 testdb.py
```

**6. Set Up PDF Documents**
```bash
mkdir -p data/pdfs
# Add your PDF documents to data/pdfs/
# Then run:
python3 scripts/ingest_pdfs.py
```

**7. Start Backend**
```bash
uvicorn api.main:app --reload
```

**8. Test API** (in new terminal)
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"text":"hello"}'
```

**9. Start Frontend** (in another terminal with `.venv` activated)
```bash
streamlit run frontend.py
```

---

## ğŸ“š Implementation Guide

Complete the Quick Start setup above!

---

### ğŸ“… DAY 1: Database Setup & AI Fallback

**ğŸ¯ Learning Objectives:**
- Understand the 3-level intent classification system
- Learn database structure and queries
- Implement AI fallback for graceful error handling

#### ğŸ” The Problem
How does the system know what type of question was asked?

**Solution:** Progressive complexity with fallback

#### ğŸ† Three-Level Classification System

##### **Level 1: Keyword Matching** âš¡ (Fast - 0.001s)
- **Handles:** 70% of queries
- **Method:** Simple word detection
- **Function:** `classify_keyword()` in `classifier.py`

**Examples:**
- âœ… "Roy canteen phone" â†’ Keywords found â†’ `db_contact`
- âœ… "Where is AB-301?" â†’ Keywords found â†’ `db_location`
- âŒ "I need to contact the mess" â†’ No exact keywords

##### **Level 2: Machine Learning** âš¡âš¡ (Medium - 0.01s)
- **Handles:** 25% of queries
- **Method:** TF-IDF + Logistic Regression
- **Training:** Pre-trained on 200+ example queries
- **Function:** `ml_classify()` in `classifier.py`

**How it works:**
- Converts text to numerical features (word importance)
- Trained model predicts intent
- Example: "mess contact" â†’ ML recognizes as contact query

**When it works:**
- âœ… Variations of known patterns
- âœ… Synonyms and paraphrases

**When it fails:**
- âŒ Completely novel phrasing
- âŒ Ambiguous questions

##### **Level 3: LLM Classification** âš¡âš¡âš¡ (Slow ~ 1-2s) [HW]

**Hints:**
- Sends query to Mistral-7B with instructions
- "Classify this as: contact/location/rag/small_talk/fallback"
- Returns intent with reasoning

**Example:**
- "Can you help me reach the person in charge of food services?" â†’ LLM understands context â†’ `db_contact`

#### ğŸ—„ï¸ Database Structure

**How to create the DB:**
1. Create table models in `models.py`
2. Use `session.py` to connect
3. Populate db by forming `testdb.py`

**What's in the Database?**
- 8-10 tables in SQLite: Faculty, Canteen, Warden, Room, Building, etc.
- Fixed schema (columns known in advance)
- Fast exact matches

**Key Functions:**
- `try_get_contact(text, session)` - Search people/places
- `try_get_location(text, session)` - Search rooms/buildings
- `extract_entity_names()` - Parse query for names

#### ğŸ›¡ï¸ AI Fallback System

**Concept:** Graceful handling of out-of-scope queries

**When Fallback Triggers:**
- Intent classified as "ai_fallback"
- Database search returns nothing
- RAG search finds no relevant documents
- Confidence too low (< 0.3)

**Response Generation:**
- Send query + system prompt to Mistral-7B
- Temperature: 0.7 (more creative for deflection)
- AI generates polite refusal + redirection + organized response

**Key Function:** `fallback_ai_response(query)` in `fallback_message.py`

#### ğŸ§ª Hands-on Exercise

Edit `testdb.py` and add/populate data of your choice

**Functions Involved:**
- `session.add()` - Add to database
- `session.commit()` - Save changes
- `try_get_contact()` - Search function

---

### ğŸ“… DAY 2: RAG (Retrieval Augmented Generation)

**ğŸ¯ Learning Objectives:**
- Understand RAG (Retrieval Augmented Generation) concept
- Learn PDF processing pipeline (extraction â†’ chunking â†’ embedding)
- Understand semantic search and cosine similarity

#### ğŸ¤” The Problem

**Student asks:** "How to calculate CGPA?"

**Why Database Won't Work:**
- âŒ CGPA calculation is a multi-step explanation (not a single data point)
- âŒ Rules are in PDF documents (Academic Handbook, 50+ pages)
- âŒ Manual data entry = tedious + error-prone
- âŒ Rules change â†’ Need to update database every time

**Why not raw AI?**
- âŒ LLMs hallucinate
- âŒ No existing knowledge of the campus rules
- âŒ CGPA varies from campus to campus

#### ğŸ”„ RAG Pipeline

**Document Processing:**
```
PDFs â†’ Extract Text â†’ Split into Chunks â†’ Convert to Vectors â†’ Store in Database
```

**Query Processing:**
```
User Question â†’ Convert to Vector â†’ Find Similar Vectors â†’ Get Text Chunks
Question + Context Chunks â†’ LLM â†’ Natural Answer
```

#### ğŸ“„ PDF Processing Pipeline

##### **1. PDF Text Extraction**

**Process:**
1. Open PDF file
2. Iterate through each page
3. Extract text layer (embedded text data)
4. Concatenate all pages

**Key Function:** `extract_text_pypdf2()` in `pdf_processor.py`

##### **2. Text Cleaning**

**Removes Noise:**
- Page numbers
- URLs
- Headers/footers
- Extra whitespaces

**Key Function:** `clean_text()` in `pdf_processor.py`

##### **3. Quality Check**

Check the length of answer and whether it is in readable format

**Key Function:** `validate_extracted_text()` in `pdf_processor.py`

#### âœ‚ï¸ Text Chunking

**Why Chunking?**
- Embedding models have token limits for each query
- Semantic search less accurate with more tokens
- Higher API cost with larger inputs

**Solution:** Split into smaller, semantically meaningful pieces

**Chunking Parameters:**
- `chunk_size`
- `chunk_overlap`
- `min_chunk_size`

**Key Function:** `chunk_text()` in `chunking.py`

#### ğŸ”¢ Embeddings

**Model Used:** `all-MiniLM-L6-v2` (Sentence Transformers)

**Concept:** Convert text into numbers that capture meaning

**Example:**
```
Text: "How to calculate CGPA?"
Embedding: [0.234, -0.112, 0.567, ..., 0.891]  (384 numbers)

"CGPA calculation" â†’ [0.12, 0.45, -0.23, ...]
"Grade point average" â†’ [0.15, 0.43, -0.20, ...]  (CLOSE! âœ…)
"Pizza recipe" â†’ [0.87, -0.32, 0.61, ...]  (FAR! âŒ)
```

**Key Functions:**
- `get_embeddings()` in `embeddings.py`
- `generate_embeddings()` in `embeddings.py`

#### ğŸ—ƒï¸ Vector Database - ChromaDB

Stores all the embeddings for semantic search

#### ğŸ§ª Hands-on Exercise

```bash
# 1. Add PDF to data folder
cp ~/hostel_rules.pdf data/pdfs/

# 2. Run ingestion
python3 scripts/ingest_pdfs.py

# 3. Check ChromaDB
python3 -c "from core.rag import collection; print(f'{collection.count()} documents')"

# 4. Test query
curl -X POST http://localhost:8000/api/chat \
  -d '{"text":"What are hostel visiting hours?"}'
```




# ========================================================================
# DAY 3 : Classifier
# ========================================================================

Problem: When a user asks "Roy canteen phone", how does the system know they want contact information and not location or rules?

Solution: Intent Classification - categorizing user queries into predefined intents.

# Intent Types in Campus Companion:
db_contact - Contact information (phone, email)
db_location - Location queries (rooms, buildings)
rag - Document-based questions (CGPA rules, policies)
ai_fallback - General questions / greetings
small_talk [HW]

# Three_Level Classification:
Keyword matching (Fast) -> Machine Learning (Accurate) -> LLM (Slow but most Accurate) [HW]

# Key Functions to discuss:
classify_keywords(text: str) -> IntentResult

Purpose: Fast rule-based classification using keyword matching

# Priority order matters!!!!!!
1. Check for RAG keywords â†’ "CGPA", "rules", "policy"
2. Check for contact keywords â†’ "phone", "email", "canteen"
3. Check for location keywords â†’ "where", "room", "building"
4. Default â†’ ai_fallback

# Why?
RAG first because academic queries are most specific
Contact/Location second because they have clear entities
Fallback last as catch-all


# Concept of ML
# Key Function:
MLClassifier Class

Purpose: Learn patterns from training examples using Machine Learning
Understand in brief:
1. TF-IDF Vectorizer
2. Logistic Regression

# The Orchestrator
UnifiedClassifier.classify()
Purpose: Combine all three classifiers and make final decision

# Classification Pipeline: 
Step 1: Run keyword classifier (always)
  â†“
Step 2: Run ML classifier (if trained)
  â†“
Step 3: Run LLM classifier (if requested AND confidence < 0.7)
  â†“
Step 4: Aggregate results by taking MAX confidence per intent
  â†“
Step 5: Detect multi-intent queries
  â†“
Step 6: Determine if AI fallback needed
  â†“
Return ClassificationResult

# Result Aggregation Strategy: 
Why MAX (not AVG)?

If one classifier is very confident, it likely found a strong signal
Average would dilute strong predictions
Example: Keyword (0.90) + ML (0.60) â†’ MAX=0.90 (better than AVG=0.75)

[HW] Multi-intent Discussion




# ========================================================================
# DAY 4 : Response Generation + Frontend
# ========================================================================
Understand how raw data is converted to natural language responses
Learn the role of AI in response formatting
Understand the frontend-backend connection

The Problem: Database returns raw data for "Roy canteen phone"
Raw Output:
name: Roy Canteen
phone: +91-8012345678
email: roy@campus.edu
location: Ground Floor

User Experience: âŒ Boring, mechanical, not conversational
What Users Expect: âœ… Natural, helpful, human-like response

The Solution: AI Response Formatter
ğŸ½ï¸ Roy Canteen

You can reach Roy Canteen at +91-8012345678 or email them at roy@campus.edu. They're located on the Ground Floor!

# Flow: 
RAW DATA (from DB/RAG)
    â†“
AI FORMATTER (response.py)
    â†“
NATURAL LANGUAGE RESPONSE
    â†“
FRONTEND (frontend.py)
    â†“
USER SEES POLISHED ANSWER

# AI Response Formatter 
Architecture Overview
# Key Classes:
ResponseGenerator in response.py

# Steps:
1. __init__() - Initialization
Purpose: Set up LLM (Mistral-7B) and RAG system
2. refine_query(query: str) -> str
Purpose: Improve search queries before RAG lookup
3. format_response(query: str, data: str) -> str
Purpose: Convert raw data to natural language
4. generate_rag_response(query: str) -> Dict
Purpose: Complete RAG pipeline - search docs + generate answer

Helper Functions : 
=====>_build_context(documents, max_length) **

Combines document chunks into one string
Stops at 2000 chars (LLM context limit)
Labels each source: [Source 1], [Source 2], etc.
=====>_generate_llm_answer(query, context)

Sends context + query to Mistral-7B
Prompt engineering: "Answer using ONLY context"
Prevents hallucination (AI making up facts)
=====>_calculate_confidence(documents)

Average relevance score of top 3 chunks
Example: (0.92 + 0.87 + 0.81) / 3 = 0.87
=====>_format_sources(documents)

Extract metadata: filename, relevance score
Show users where answer came from (transparency)

5. _generate_contact_response(query: str) -> Dict
Purpose: Format database contact results
6. _generate_location_response(query: str) -> Dict
Purpose: Format database location results
7. _generate_ai_fallback_response(query: str) -> Dict
Purpose: Handle out-of-scope queries gracefully
8. generate_response(query: str, intent: str) -> Dict
Purpose: Main entry point - routes to correct handler


# Frontend -> frontend.py (based on StreamLit)
What is Streamlit?
Streamlit = Python web framework for data apps

Why Streamlit?
Write web UI in pure Python (no HTML/CSS/JavaScript)
Auto-refreshes on code changes
Built-in chat components (st.chat_message, st.chat_input)
Fast prototyping (build UI in 50 lines!)

# Key components explained:
1. Page configuration : st.set_page_config
2. Sidebar : st.sidebar
3. Session State : Converstation memory and Chat History [HW]
4. Chat Input & API Call : st.chat_input

# Start backend
uvicorn api.main:app --reload
# Start frontend (on another terminal with .venv activated)
streamlit run frontend.py


# Interaction
Common Questions
Q: Why separate frontend and backend?
A: Scalability. Backend can serve multiple frontends (web, mobile, API users).

Q: Can we use React instead of Streamlit?
A: Yes! Backend API is framework-agnostic. Just POST to /api/chat.

Q: Why not format in chat.py directly?
A: Separation of concerns. response.py is reusable across different endpoints.

Q: How to deploy to production?
A: Backend â†’ Railway/Render. Frontend â†’ Streamlit Cloud (free tier).





# ========================================================================
# DAY 5+6 : Chat System + FastAPI
# ========================================================================

# main.py (25-30min)
Goal: Students understand what this file does, why each part exists, and how requests flow, without learning FastAPI itself.

# What is main.py
main.py is the entry point of the backend.
It starts the server, prepares the environment, and connects all routes.

Key idea:
Nothing intelligent happens here
It does not answer questions
It sets up everything needed so other files can work

=====> app = FastAPI(...)
app is the control center of the backend
Every endpoint, rule, and config is attached to it

=====> CORS Middleware Block
Frontend and backend usually run on different ports
Browsers block such requests by default

CORS â†’ Browser security rule
Middleware â†’ Code that runs before requests reach routes

allow_origins â†’ Who can access the backend
allow_methods â†’ What HTTP actions are allowed
allow_headers â†’ What headers are accepted
allow_credentials â†’ Whether cookies/auth can pass

=====> init_db()
Database tables exist before any request
Backend never crashes due to missing tables

Reads database models
Creates tables if missing
Skips if already present

=====> app.include_router(...)
A router is a group of related endpoints
Example: chat routes live in chat.py

Connects /api/chat â†’ logic in chat.py
Adds structure and modularity

======> Root Endpoint /
Helpful for:
Debugging
Deployment checks
Dev sanity checks

=======> Health check /health
Every production backend has a health endpoint
It answers only one thing:
â€œAm I alive?â€



# chat.py
chat.py is where user input becomes an intelligent response.

What this file is responsible for:

Receiving user queries
Validating input
Classifying intent
Fetching data (DB / RAG)
Using AI when needed
Returning a structured response

# Chat Endpoint: /api/chat
Role of the endpoint:
Single entry point for all user queries
Handles:
Simple greetings
Database lookups
Document-based questions
AI fallback responses

Why one endpoint?
Simplifies frontend
Centralizes logic
Easier to debug and extend

# Key function to mention
chat(request: ChatRequest)

Explain:
This function is the orchestrator â€” it doesnâ€™t do everything itself, but controls everything.

# Request & Response Models
ChatRequest

Purpose:
Guarantees valid input
Prevents malformed data
Makes API predictable

ChatResponse

Purpose:
Standardizes backend output
Makes frontend rendering easy

answer â†’ Final message
intent â†’ What the system understood
confidence â†’ How sure the system is
used_fallback â†’ Whether AI was used
is_multi_intent â†’ Multiple meanings detected
all_intents â†’ Ranked intent candidates

# Intent Classification Pipeline
# Key function : classify_detailed

The system decides what the user wants, not how to answer yet

==> Types of intents used:

db_contact
db_location
faculty_info
rag
small_talk [HW][greetings]
ai_fallback

Initial classification helps to avoid unnecessary DB calls and prevents wrong answers

==> Important classification outputs

primary_intent
confidence
needs_fallback
is_multi_intent
all_intents

-------------day 6-----------------------


# Handlers, Data Retrieval & AI Fallback(returning fixed answer in our case)
Main routing decision block : Based on primary_intent

==> Important Handler Functions:
try_get_contact()
try_get_location()
try_get_faculty()
try_get_rag()
fallback_ai_response()

# Understand each functions and interactive session (as all functions contain concept of past topics)

# Response formatting
Final step before your query is sent, processed through a no. of steps and ready to be printed in JSON format -> formatting is required to return in user-friendly form



```
Dear Students,

Over the past 6 days, we built Campus Companion, an AI-powered chatbot that helps students find contact information, locations, and academic policies through a beautiful Streamlit interface. The system uses a 3-layer architecture: Frontend (Streamlit for UI), Backend (FastAPI for API server), and Core Intelligence (classification, database handlers, RAG, and AI formatting). When a user asks "Roy canteen phone", the request flows through Pydantic validation â†’ 3-level intent classification (keywords/ML/LLM) â†’ routing to appropriate handler (try_get_contact searches the database with fuzzy matching) â†’ AI formatting (Mistral-7B converts raw data to natural language) â†’ structured JSON response displayed in the frontend. We used modern technologies including FastAPI (REST API), SQLAlchemy (database ORM), Scikit-learn (ML classification), ChromaDB (vector database for RAG), HuggingFace (embeddings and LLM), and followed production-grade principles like separation of concerns, graceful degradation (fallback mechanisms), comprehensive error handling, type safety with Pydantic, and extensive logging. The key innovation is our hybrid approach: combining structured database queries for contacts/locations with RAG (Retrieval-Augmented Generation) for document-based questions like "How to calculate CGPA?", where we use semantic search to find relevant PDF chunks and generate contextual answers. You've learned full-stack development (frontend + backend + database), AI/ML integration (classification, embeddings, LLMs), software engineering (clean architecture, error handling, API design), and most importantly, you've built a real-world application that solves actual campus problems. This same architecture can be adapted for hospital assistants, corporate helpdesks, e-commerce support, or any domain requiring intelligent information retrieval. You're now ready to extend this system (add new intents, multilingual support), improve accuracy (fine-tune classifiers, better RAG strategies), deploy to production (Railway/Render/AWS), and add advanced features (voice input, analytics dashboards). Congratulations - you didn't just learn to code, you learned to think like a software engineer, understanding why each component exists, how they communicate, and when to use different approaches, demonstrating skills that companies actively seek in full-stack AI developers. Now go build something amazing! ğŸš€

You've mastered: FastAPI + Streamlit + SQLAlchemy + ChromaDB + HuggingFace + RAG + Clean Architecture

Keep coding, keep learning, keep building! ğŸ’™
```