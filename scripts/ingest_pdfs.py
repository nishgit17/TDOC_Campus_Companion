"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     PDF INGESTION PIPELINE                               â•‘
â•‘         Complete Pipeline: PDF â†’ Text â†’ Chunks â†’ Embeddings              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ FILE ROLE IN PROJECT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
This is the DATA INGESTION SCRIPT for the Campus Companion RAG system.
It processes PDF documents and creates searchable embeddings in ChromaDB.

ğŸ”— HOW IT FITS IN THE ARCHITECTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  COMPLETE INGESTION FLOW                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [INPUT] PDF Files (data/pdfs/)                                     â”‚
â”‚      â€¢ academic_rules.pdf                                           â”‚
â”‚      â€¢ hostel_guidelines.pdf                                        â”‚
â”‚      â€¢ exam_regulations.pdf                                         â”‚
â”‚       â†“                                                             â”‚
â”‚  [STEP 1] Extract Text (PDFProcessor)                               â”‚
â”‚      â€¢ Try text extraction first (fast)                             â”‚
â”‚      â€¢ Fall back to OCR if needed (slow but accurate)               â”‚
â”‚      â€¢ Output: Plain text strings                                   â”‚
â”‚       â†“                                                             â”‚
â”‚  [STEP 2] Chunk Text (TextChunker)                                  â”‚
â”‚      â€¢ Split into 512-word chunks                                   â”‚
â”‚      â€¢ Add 50-word overlap                                          â”‚
â”‚      â€¢ Preserve metadata (filename, pages)                          â”‚
â”‚       â†“                                                             â”‚
â”‚  [STEP 3] Generate Embeddings (THIS FILE)                           â”‚
â”‚      â€¢ Use sentence-transformers (all-MiniLM-L6-v2)                 â”‚
â”‚      â€¢ Convert each chunk â†’ 384-dim vector                          â”‚
â”‚      â€¢ ChromaDB handles this automatically!                         â”‚
â”‚       â†“                                                             â”‚
â”‚  [STEP 4] Store in ChromaDB (data/rag_docs/)                        â”‚
â”‚      â€¢ Persistent vector database                                   â”‚
â”‚      â€¢ Fast similarity search                                       â”‚
â”‚      â€¢ Ready for RAG queries!                                       â”‚
â”‚       â†“                                                             â”‚
â”‚  [OUTPUT] Searchable Knowledge Base                                 â”‚
â”‚      â€¢ Used by: core/rag.py                                         â”‚
â”‚      â€¢ Powers: "How to calculate CGPA?" queries                     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ WHAT THIS SCRIPT DOES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Scans data/pdfs/ directory for PDF files
2. Extracts text from each PDF (with OCR fallback)
3. Chunks text into 512-word segments with overlap
4. Generates embeddings using sentence-transformers
5. Stores in ChromaDB for semantic search
6. Displays statistics (documents processed, chunks created)

ğŸš€ HOW TO RUN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Method 1 - Run directly:
    python3 scripts/ingest_pdfs.py

Method 2 - From project root:
    python3 -m scripts.ingest_pdfs

What happens:
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Starting PDF Ingestion Pipeline
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    [1/4] Extracting text from PDFs...
    âœ“ academic_rules.pdf - 2,500 words (text extraction)
    âœ“ hostel_guidelines.pdf - 1,800 words (OCR)
    âœ“ exam_regulations.pdf - 3,200 words (text extraction)
    
    [2/4] Chunking text...
    âœ“ Created 15 chunks (avg: 512 words/chunk)
    
    [3/4] Preparing documents for embedding...
    âœ“ Ready for ChromaDB ingestion
    
    [4/4] Storing in ChromaDB...
    âœ“ Successfully added 15 documents
    âœ“ Collection now has 15 documents
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Ingestion Complete! âœ…
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š EXAMPLE TRANSFORMATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input: academic_rules.pdf (10 pages, 5000 words)

After Processing:
  â”œâ”€ Chunk 1 (512 words)
  â”‚    Text: "CGPA Calculation Rules: The cumulative..."
  â”‚    Embedding: [0.234, -0.156, 0.891, ...] (384 dims)
  â”‚    Metadata: {filename: 'academic_rules.pdf', pages: 10}
  â”‚
  â”œâ”€ Chunk 2 (512 words)
  â”‚    Text: "...grade point average is calculated..."
  â”‚    Embedding: [0.445, 0.223, -0.334, ...] (384 dims)
  â”‚    Metadata: {filename: 'academic_rules.pdf', pages: 10}
  â”‚
  â””â”€ ... (8 more chunks)

Stored in ChromaDB:
  â€¢ Fast similarity search
  â€¢ Automatically indexed
  â€¢ Query: "how is CGPA calculated?"
    â†’ Returns: Chunks 1, 2 (highest similarity scores)

ğŸ”§ CONFIGURATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PDF_DIR = "data/pdfs/"
  â€¢ Where to find PDF files
  â€¢ Can be changed via constructor parameter

DB_PATH = "data/rag_docs/"
  â€¢ Where ChromaDB stores data
  â€¢ Persistent storage (survives restarts)

COLLECTION_NAME = "campus_docs"
  â€¢ Name of ChromaDB collection
  â€¢ Can have multiple collections for different purposes

CHUNK_SIZE = 512 words
  â€¢ How many words per chunk
  â€¢ Adjust in TextChunker initialization

CHUNK_OVERLAP = 50 words
  â€¢ Overlap between chunks
  â€¢ Prevents context loss

EMBEDDING_MODEL = "all-MiniLM-L6-v2"
  â€¢ Sentence transformer model
  â€¢ 384 dimensions, fast, good quality
  â€¢ Downloaded automatically on first run (~90MB)

âš™ï¸ COMPONENTS USED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. PDFProcessor (pdf_processor.py)
   â€¢ PyPDF2: Basic text extraction
   â€¢ pdfplumber: Better for tables
   â€¢ Tesseract OCR: For scanned PDFs
   
2. TextChunker (chunking.py)
   â€¢ Word-based sliding window
   â€¢ Preserves metadata
   
3. ChromaDB
   â€¢ Vector database
   â€¢ Handles embeddings automatically
   â€¢ No manual FAISS/Pinecone setup needed
   
4. sentence-transformers
   â€¢ all-MiniLM-L6-v2 model
   â€¢ Converts text â†’ vectors
   â€¢ Managed by ChromaDB

ğŸ’¡ EMBEDDING EXPLAINED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
What is an embedding?
  â€¢ Numerical representation of text meaning
  â€¢ Each chunk â†’ 384-number vector
  â€¢ Similar meanings â†’ similar vectors

Example:
  "CGPA calculation rules"     â†’ [0.23, -0.15, 0.89, ...]
  "how to calculate grades"    â†’ [0.25, -0.14, 0.87, ...]  (similar!)
  "hostel food menu"           â†’ [-0.45, 0.67, -0.12, ...] (different)

When user asks: "How do I calculate my CGPA?"
  1. Convert query â†’ embedding [0.24, -0.16, 0.88, ...]
  2. Find chunks with similar embeddings (cosine similarity)
  3. Return top 3 most relevant chunks
  4. LLM generates answer from those chunks

ğŸ”„ RE-INGESTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
To update documents:
  1. Add/modify PDFs in data/pdfs/
  2. Run: python3 scripts/ingest_pdfs.py
  3. ChromaDB will ADD new documents (won't delete old)
  
To start fresh:
  1. Delete: data/rag_docs/ folder
  2. Run: python3 scripts/ingest_pdfs.py
  3. Clean ChromaDB created from scratch

ğŸ“ IMPORTANT NOTES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ First run downloads embedding model (~90MB) - be patient!
â€¢ OCR requires tesseract-ocr installed: brew install tesseract
â€¢ Large PDFs take time (1-2 mins for 100-page PDF with OCR)
â€¢ ChromaDB is persistent - data survives script restarts
â€¢ Safe to run multiple times (adds new docs, doesn't duplicate)

âš ï¸ TROUBLESHOOTING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Error: "No such file or directory: data/pdfs"
  â†’ Create folder: mkdir -p data/pdfs
  â†’ Add some PDF files

Error: "tesseract not found"
  â†’ Install: brew install tesseract (macOS)
  â†’ Or disable OCR: PDFProcessor(ocr_enabled=False)

Error: "ChromaDB initialization failed"
  â†’ Delete data/rag_docs/ and try again
  â†’ Check permissions

Error: "Out of memory"
  â†’ Process PDFs in smaller batches
  â†’ Reduce chunk_size or process fewer files
"""


# ===========================================================================
# IMPORTS
# ===========================================================================



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADD PROJECT ROOT TO PYTHON PATH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDARD LIBRARIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOGGING CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PDF TO VECTOR DB INGESTION PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




# ===========================================================================
# RUN SCRIPT
# ===========================================================================
