"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     TEXT CHUNKING FOR RAG                                â•‘
â•‘              Splits Documents into Searchable Chunks                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ FILE ROLE IN PROJECT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
This file splits long documents into smaller CHUNKS for RAG (Retrieval Augmented Generation).
It's a critical preprocessing step before creating embeddings.

ğŸ”— HOW IT FITS IN THE ARCHITECTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA INGESTION PIPELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [1] PDF FILES (data/pdfs/)                                         â”‚
â”‚      â€¢ academic_rules.pdf (50 pages, 20,000 words)                  â”‚
â”‚      â€¢ hostel_guidelines.pdf (30 pages, 12,000 words)               â”‚
â”‚       â†“                                                             â”‚
â”‚  [2] PDF PROCESSOR (scripts/pdf_processor.py)                       â”‚
â”‚      â€¢ Extracts text from PDFs                                      â”‚
â”‚      â€¢ Output: Full text strings                                    â”‚
â”‚       â†“                                                             â”‚
â”‚  [3] THIS FILE (scripts/chunking.py) â† YOU ARE HERE!                â”‚
â”‚      â€¢ Splits text into 512-word chunks                             â”‚
â”‚      â€¢ Creates 50-word overlap between chunks                       â”‚
â”‚      â€¢ Output: List of chunks with metadata                         â”‚
â”‚       â†“                                                             â”‚
â”‚  [4] EMBEDDING GENERATION (scripts/ingest_pdfs.py)                  â”‚
â”‚      â€¢ Converts each chunk to 384-dim vector                        â”‚
â”‚       â†“                                                             â”‚
â”‚  [5] CHROMADB STORAGE (data/rag_docs/)                              â”‚
â”‚      â€¢ Stores chunks + embeddings                                   â”‚
â”‚      â€¢ Ready for semantic search!                                   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ WHY CHUNKING IS NECESSARY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: LLMs have context limits
  â€¢ Mistral-7B: ~8000 tokens (~6000 words)
  â€¢ Can't process 50-page PDF all at once
  
Solution: Break into smaller chunks
  â€¢ Each chunk is self-contained
  â€¢ Overlap ensures context isn't lost
  â€¢ Search returns only relevant chunks

Example:
  Original: 20,000-word PDF
  After chunking: 40 chunks Ã— 512 words each
  Query: "CGPA calculation"
  Search returns: 3 most relevant chunks (1536 words total)
  LLM processes: Only those 3 chunks, not entire PDF!

ğŸ’¡ CHUNKING STRATEGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Word-Based Sliding Window:
  
  CHUNK 1: Words 0-512
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ CGPA is calculated by summing all...    â”‚
  â”‚ ...grade points divided by credits...   â”‚
  â”‚ ...following rules apply: (1) minimum   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ (overlap 50 words)
  CHUNK 2: Words 462-974
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ...following rules apply: (1) minimum   â”‚
  â”‚ ...passing grade is D, (2) grades are   â”‚
  â”‚ ...recorded per semester, (3) final...  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits of Overlap:
  â€¢ Ensures important sentences aren't cut off
  â€¢ Maintains context across chunks
  â€¢ If query matches boundary, both chunks retrieved

ğŸ”§ CONFIGURATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
chunk_size = 512 words
  â€¢ ~384-512 English words
  â€¢ ~2500-3000 characters
  â€¢ Sweet spot for semantic search
  
chunk_overlap = 50 words
  â€¢ ~10% overlap
  â€¢ Balances context vs redundancy
  â€¢ Prevents important info loss at boundaries

ğŸ“Š CHUNKING EXAMPLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input Text (1000 words):
  "CGPA Calculation Rules: The CGPA is calculated by dividing the sum
   of grade points by total credits. Grade points for each course are
   computed by multiplying the grade value by the course credits..."

Output Chunks:
  Chunk 1 (words 0-512):
    text: "CGPA Calculation Rules: The CGPA is..."
    metadata: {filename: 'academic_rules.pdf', page: 1}
    length: 512 words

  Chunk 2 (words 462-974):  # Overlaps by 50 words
    text: "...grade points by total credits. Grade points..."
    metadata: {filename: 'academic_rules.pdf', page: 1-2}
    length: 512 words

  Chunk 3 (words 924-1000):  # Last chunk (shorter)
    text: "...computed by multiplying the grade value..."
    metadata: {filename: 'academic_rules.pdf', page: 2}
    length: 76 words

ğŸ’» USAGE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from scripts.chunking import TextChunker
    
    # Initialize chunker
    chunker = TextChunker(chunk_size=512, chunk_overlap=50)
    
    # Chunk a document
    text = "Long document text here..."
    metadata = {'filename': 'rules.pdf', 'page': 1}
    chunks = chunker.chunk_text(text, metadata)
    
    # Result:
    # [
    #   {'text': '...', 'metadata': {...}, 'length': 512},
    #   {'text': '...', 'metadata': {...}, 'length': 512},
    #   ...
    # ]

âš™ï¸ HOW TO ADJUST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Larger Chunks (More Context):
    chunker = TextChunker(chunk_size=1024, chunk_overlap=100)
    â†‘ Better for complex queries needing more context
    â†“ But: More tokens used, slower search

Smaller Chunks (More Precise):
    chunker = TextChunker(chunk_size=256, chunk_overlap=25)
    â†‘ Better for specific factual queries
    â†“ But: May lose context, more chunks to search

ğŸ“ NOTES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Chunks are measured in WORDS, not characters
â€¢ Text is cleaned (extra whitespace removed)
â€¢ Metadata preserved for traceability
â€¢ If text < chunk_size, returns single chunk
â€¢ Overlap prevents infinite loop (capped at chunk_size-1)
"""

# ===========================================================================
# IMPORTS
# ===========================================================================



# ===========================================================================
# TEXT CHUNKER CLASS
# ===========================================================================
