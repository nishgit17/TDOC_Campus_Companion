# =============================== DAY - 2 ================================ #
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          RAG (RETRIEVAL AUGMENTED GENERATION) SYSTEM                     â•‘
â•‘        Semantic Search and Document Retrieval for Campus AI              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ FILE ROLE IN PROJECT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
This is the QUERY ENGINE of the Campus Companion RAG system.
It performs semantic search to find relevant document chunks for user questions.

This file is the FINAL STEP in the teaching sequence - where RAG actually happens!

ğŸ”— HOW IT FITS IN THE ARCHITECTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   COMPLETE RAG ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [INGESTION PHASE] - Done Once                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚   1. PDF files (data/pdfs/)                                         â”‚
â”‚   2. Extract text (scripts/pdf_processor.py)                        â”‚
â”‚   3. Chunk text (scripts/chunking.py)                               â”‚
â”‚   4. Generate embeddings (core/embeddings.py)                       â”‚
â”‚   5. Store in ChromaDB (scripts/ingest_pdfs.py)                     â”‚
â”‚                                                                     â”‚
â”‚  [QUERY PHASE] - Every User Request (THIS FILE!)                    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚   1. User asks: "How to calculate CGPA?"                            â”‚
â”‚   2. Convert question â†’ embedding (384-dim vector)                  â”‚
â”‚   3. Search ChromaDB for similar embeddings                         â”‚
â”‚   4. Retrieve top-k most relevant chunks                            â”‚
â”‚   5. Return chunks to LLM                                           â”‚
â”‚   6. LLM generates answer from retrieved context                    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ WHAT IS RAG?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAG = Retrieval Augmented Generation

Problem: LLMs don't know your specific information
  â€¢ Campus rules change every semester
  â€¢ Hostel policies are unique to your college
  â€¢ LLM training data doesn't include this

Traditional Solutions (Don't Work Well):
  âŒ Fine-tuning: Expensive, slow, needs retraining for updates
  âŒ Prompt stuffing: Hit context limits with large docs
  âŒ Manual updates: Time-consuming, error-prone

RAG Solution (Best Approach):
  âœ… Store documents as searchable embeddings
  âœ… Find relevant chunks on-demand
  âœ… Give only relevant context to LLM
  âœ… Update by re-ingesting documents (no retraining!)

ğŸ’¡ RAG QUERY PROCESS (STEP BY STEP):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Let's trace a real query: "How to calculate CGPA?"

STEP 1: User Query
  Input: "How to calculate CGPA?"

STEP 2: Query Embedding
  â€¢ Convert query to 384-dim vector using same model as ingestion
  â€¢ Query vector: [0.234, -0.156, 0.891, ...]
  â€¢ Model: all-MiniLM-L6-v2 (same as embedding phase)

STEP 3: Similarity Search in ChromaDB
  â€¢ Compare query vector with all stored chunk vectors
  â€¢ Calculate cosine similarity: similarity = dot(query, chunk) / (|query| * |chunk|)
  â€¢ Similarity ranges from 0 (unrelated) to 1 (identical meaning)

  Example results:
    Chunk 1: "CGPA calculation involves..." â†’ similarity: 0.89 â­â­â­
    Chunk 2: "Grade points are computed..." â†’ similarity: 0.76 â­â­
    Chunk 15: "Hostel food menu includes..." â†’ similarity: 0.12 âŒ

STEP 4: Retrieve Top-K Chunks
  â€¢ Sort by similarity score
  â€¢ Take top 3 chunks (configurable)
  â€¢ Filter by minimum score (default: 0.3)

STEP 5: Format Results
  Return:
  [
    {'content': 'CGPA calculation...', 'relevance_score': 0.89},
    {'content': 'Grade points...', 'relevance_score': 0.76},
    {'content': 'Final CGPA...', 'relevance_score': 0.68}
  ]

STEP 6: LLM Answer Generation (core/response.py)
  â€¢ Combine retrieved chunks into context
  â€¢ Send to LLM: "Based on: [chunks], answer: [query]"
  â€¢ LLM generates accurate, context-aware response

ğŸ” SEMANTIC SEARCH EXPLAINED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Semantic = Understanding Meaning, Not Just Keywords

Traditional Keyword Search:
  Query: "CGPA calculation"
  Matches: Only documents with exact words "CGPA" and "calculation"
  Misses: "How to compute grade point average"

Semantic Search (Embedding-Based):
  Query: "CGPA calculation" â†’ [0.23, -0.15, 0.89, ...]
  Finds similar meanings:
    âœ“ "CGPA calculation" (exact match)
    âœ“ "How to compute grade point average" (same meaning!)
    âœ“ "GPA computation rules" (related concept)
    âœ“ "Academic performance metrics" (broader topic)

This is why embeddings are powerful!

ğŸ“Š EXAMPLE QUERY FLOW:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: "Can I change my hostel?"

1. Query Embedding:
   "Can I change my hostel?" â†’ [0.45, 0.23, -0.67, ...]

2. ChromaDB Search (behind the scenes):
   â€¢ Compares with 150 stored chunks
   â€¢ Finds top 3 matches:
     
     Rank 1 (score: 0.87):
       "Hostel changes are allowed after first semester.
        Students must submit request form to warden..."
     
     Rank 2 (score: 0.72):
       "Room swapping procedure: Fill form at hostel office.
        Approval takes 2-3 weeks..."
     
     Rank 3 (score: 0.65):
       "Hostel allocation policy: First-years assigned randomly.
        Second-years can request preferred hostel..."

3. Return to LLM:
   LLM receives these 3 chunks and generates:
   "Yes, you can change your hostel after the first semester.
    You need to submit a request form to the warden..."

ğŸ’» USAGE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from core.rag import RAGSystem, get_rag_system
    
    # Initialize RAG system (connects to ChromaDB)
    rag = RAGSystem()
    
    # Search for relevant documents
    results = rag.search_documents(
        query="How to calculate CGPA?",
        top_k=3,           # Return top 3 chunks
        min_score=0.3      # Minimum relevance threshold
    )
    
    # Use results
    for doc in results:
        print(f"Score: {doc['relevance_score']:.2f}")
        print(f"Content: {doc['content'][:100]}...")
    
    # Or use singleton instance (recommended for API)
    rag = get_rag_system()  # Reuses same instance
    results = rag.search_documents("hostel rules")

ğŸ”§ CONFIGURATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
db_path = "data/rag_docs"
  â€¢ Where ChromaDB stores data
  â€¢ Persistent storage survives restarts

collection_name = "campus_docs"
  â€¢ Name of document collection
  â€¢ Can have multiple collections for different purposes

top_k = 3
  â€¢ Number of chunks to retrieve
  â€¢ 3-5 is usually optimal
  â€¢ Too few: might miss context
  â€¢ Too many: adds noise, uses more tokens

min_score = 0.3
  â€¢ Minimum relevance threshold (0-1)
  â€¢ 0.3 is permissive (broader results)
  â€¢ 0.6 is strict (only high-quality matches)
  â€¢ Adjust based on your needs

embedding_model = "all-MiniLM-L6-v2"
  â€¢ MUST match ingestion model!
  â€¢ 384 dimensions
  â€¢ Fast and accurate

âš¡ PERFORMANCE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query Speed:
  â€¢ Small collection (100 chunks): ~10-50ms
  â€¢ Medium collection (1000 chunks): ~50-200ms
  â€¢ Large collection (10,000 chunks): ~200-500ms
  â€¢ ChromaDB uses HNSW index for fast search

Memory Usage:
  â€¢ Embeddings stay in ChromaDB (not in RAM)
  â€¢ Only loads what's needed for query
  â€¢ Efficient for large document sets

Scalability:
  â€¢ ChromaDB handles millions of documents
  â€¢ For production: consider managed ChromaDB or Pinecone
  â€¢ Can implement caching for frequent queries

ğŸ“ WHY RAG OVER ALTERNATIVES?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. vs Fine-Tuning:
   RAG: Update documents anytime, instant effect
   Fine-Tuning: Retrain entire model, expensive, slow

2. vs Prompt Stuffing:
   RAG: Only retrieve relevant chunks (efficient)
   Prompt Stuffing: Send entire docs, hit context limits

3. vs Vector Similarity Search Only:
   RAG: Combines retrieval + generation
   Vector Search: Only finds docs, doesn't answer

4. vs Knowledge Graphs:
   RAG: Simpler to implement and maintain
   Knowledge Graphs: Complex setup, rigid structure

ğŸ“ IMPORTANT NOTES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Query embedding MUST use same model as ingestion
â€¢ ChromaDB handles embedding automatically (no manual work!)
â€¢ Singleton pattern avoids re-initializing on every query
â€¢ Distance â†’ Similarity conversion: similarity = 1 - distance
â€¢ Lower distance = higher similarity
â€¢ ChromaDB returns distances, we convert to similarity scores

âš ï¸ TROUBLESHOOTING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Error: "Collection not found"
  â†’ Run ingestion first: python scripts/ingest_pdfs.py
  â†’ Check db_path matches ingestion path

Error: "No results returned"
  â†’ Lower min_score threshold
  â†’ Check if documents were ingested properly
  â†’ Verify query is in English (model limitation)

Poor Results:
  â†’ Increase top_k (more chunks)
  â†’ Adjust min_score
  â†’ Improve document chunking (smaller/larger chunks)
  â†’ Try different embedding model

Slow Queries:
  â†’ Check collection size (too many docs?)
  â†’ Consider indexing options
  â†’ Use singleton pattern (get_rag_system())
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPORTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•





# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOGGING CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG SYSTEM CLASS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•





# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LEGACY QUERY FUNCTION FOR BACKWARD COMPATIBILITY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•



# ===========================================================================
# SINGLETON RAG SYSTEM INSTANCE
# ===========================================================================




# ===========================================================================
# GET RAG SYSTEM SINGLETON
# ===========================================================================
