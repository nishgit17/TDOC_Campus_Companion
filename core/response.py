# ================================ DAY - 4 ================================ #
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   RESPONSE GENERATION SYSTEM                             â•‘
â•‘         Final Answer Generation for Campus Companion Chatbot             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ FILE ROLE IN PROJECT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
This is the FINAL STEP in the query pipeline - generating the actual response
that users see. It takes retrieved data and converts it into natural language.

This is the "voice" of the chatbot.

ðŸ”— HOW IT FITS IN THE COMPLETE ARCHITECTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE QUERY PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [1] USER QUERY                                                     â”‚
â”‚      "How to calculate CGPA?"                                       â”‚
â”‚       â†“                                                             â”‚
â”‚  [2] INTENT CLASSIFICATION (core/classifier.py)                     â”‚
â”‚      Intent: "rag" (0.90 confidence)                                â”‚
â”‚       â†“                                                             â”‚
â”‚  [3] DATA RETRIEVAL                                                 â”‚
â”‚      RAG Search (core/rag.py) â†’ Retrieved 3 relevant chunks         â”‚
â”‚       â†“                                                             â”‚
â”‚  [4] RESPONSE GENERATION (THIS FILE!) â† YOU ARE HERE                â”‚
â”‚      Chunks + Query â†’ Natural Language Answer                       â”‚
â”‚       â†“                                                             â”‚
â”‚  [5] USER RECEIVES ANSWER                                           â”‚
â”‚      "CGPA is calculated by dividing sum of grade points..."        â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸŽ¯ WHAT THIS FILE DOES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Takes RAW DATA and converts it to USER-FRIENDLY ANSWERS:

INPUT (Raw Data):
  â€¢ RAG chunks: ["CGPA is calculated...", "Grade points are...", ...]
  â€¢ Database results: {phone: "+91-xxx", name: "Roy canteen"}
  â€¢ Intent: "rag" or "db_contact" or "db_location"

OUTPUT (Natural Language):
  â€¢ Formatted answer: "CGPA is calculated by dividing the sum..."
  â€¢ Sources: [academic_rules.pdf, student_handbook.pdf]
  â€¢ Confidence: 0.85

ðŸ”„ COMPLETE RESPONSE FLOW:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Example Query: "How to calculate CGPA?"

STEP 1: Intent Classification
  â†’ Intent: "rag" (policy question)
  â†’ Route to: RAG system

STEP 2: Query Refinement (Optional)
  Original: "How to calculate CGPA?"
  Refined: "CGPA calculation method steps"
  Why? Better semantic search results

STEP 3: RAG Search
  â†’ Search ChromaDB for: "CGPA calculation method steps"
  â†’ Retrieved chunks:
    [1] "CGPA is calculated by dividing sum of grade points..." (0.89)
    [2] "Grade points for each course are computed..." (0.76)
    [3] "Final CGPA appears on semester report..." (0.68)

STEP 4: Context Building
  â†’ Combine chunks (max 2000 chars)
  â†’ Add source labels: [Source 1], [Source 2], etc.
  â†’ Result: Single context string for LLM

STEP 5: LLM Answer Generation
  â†’ Send to Mistral-7B:
    System: "Answer only from context"
    Context: [Combined chunks]
    Query: "How to calculate CGPA?"
  
  â†’ LLM Response:
    "CGPA is calculated by dividing the sum of grade points
     by total credits. For each course, grade points are
     computed by multiplying grade value by course credits..."

STEP 6: Format & Return
  {
    "answer": "CGPA is calculated by...",
    "sources": [
      {"filename": "academic_rules.pdf", "relevance": 0.89},
      {"filename": "student_handbook.pdf", "relevance": 0.76}
    ],
    "confidence": 0.78,
    "method": "rag_hf_llm"
  }

ðŸ“Š RESPONSE METHODS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. RAG RESPONSE (Policy Questions)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Query: "How to calculate CGPA?"
   
   Process:
   1. Refine query for better search
   2. Search ChromaDB (semantic similarity)
   3. Retrieve top-k relevant chunks
   4. Build context from chunks
   5. Send to LLM for natural answer
   6. Format with sources
   
   Output:
   â€¢ Natural language explanation
   â€¢ Source documents listed
   â€¢ High confidence (0.7-0.9)

2. DATABASE CONTACT RESPONSE
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Query: "Roy canteen phone number"
   
   Process:
   1. Query contacts database
   2. Fetch: name, phone, email
   3. Format with emojis
   
   Output:
   ðŸ½ï¸ Roy Canteen
   ðŸ“ž Phone: +91-xxx-xxxx
   ðŸ“§ Email: roy@campus.edu

3. DATABASE LOCATION RESPONSE
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Query: "Where is room AB101?"
   
   Process:
   1. Query locations database
   2. Fetch: room, building, floor
   3. Format with emojis
   
   Output:
   ðŸšª Room AB101
   ðŸ¢ Building: Academic Block
   ðŸ—ï¸ Floor: 1st Floor

4. AI FALLBACK RESPONSE
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Query: "What's the weather?"
   
   Process:
   1. No database/RAG match
   2. Return capability message
   
   Output:
   "I can help with academics, contacts, and campus locations.
    Please ask something related to campus information."

ðŸ¤– LLM INTEGRATION (Mistral-7B via HuggingFace):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

WHY USE LLM?
  Raw chunks: Hard to read, fragmented
  LLM output: Natural, coherent, conversational

BEFORE LLM (Raw chunks):
  "CGPA is calculated by dividing sum grade points total credits
   Grade points computed multiplying grade value course credits
   Final CGPA appears semester report card transcript"

AFTER LLM (Natural language):
  "CGPA is calculated by dividing the sum of grade points by your
   total credits. For each course, grade points are computed by
   multiplying the grade value by the course credits. Your final
   CGPA will appear on your semester report card and transcript."

LLM CONFIGURATION:
  â€¢ Model: mistralai/Mistral-7B-Instruct-v0.2
  â€¢ API: HuggingFace Inference API (free tier)
  â€¢ Max tokens: 512 (enough for detailed answers)
  â€¢ Temperature: 0.3 (factual, not creative)
  â€¢ Timeout: 120 seconds

FALLBACK STRATEGY:
  If LLM unavailable:
  âœ“ Still works! Returns formatted chunks
  âœ— Less natural but still useful
  âœ“ No dependency on external API

âš¡ PERFORMANCE CHARACTERISTICS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Response Generation Times:
  â€¢ Database queries: ~10-50ms (instant)
  â€¢ RAG search: ~50-200ms (fast)
  â€¢ LLM generation: ~1-3 seconds (slow but acceptable)
  â€¢ Total: ~1.5-3.5 seconds for RAG queries

Optimization:
  â€¢ Singleton pattern (reuse LLM connection)
  â€¢ Context length limit (2000 chars)
  â€¢ Top-k retrieval (5 docs max)
  â€¢ Query refinement cache (future)

Token Usage (HuggingFace Free Tier):
  â€¢ Context: ~500 tokens
  â€¢ Response: ~200 tokens
  â€¢ Total: ~700 tokens per query
  â€¢ Free tier: 1000 requests/day

ðŸ”§ KEY FEATURES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. QUERY REFINEMENT
   Improves search results by rephrasing queries
   Example: "cgpa rule?" â†’ "CGPA calculation rules and requirements"

2. CONTEXT BUILDING
   Combines multiple chunks into coherent context
   Limits length to avoid token overflow
   Labels sources: [Source 1], [Source 2]

3. CONFIDENCE SCORING
   Averages relevance scores from top-3 chunks
   Range: 0.0 (no confidence) to 1.0 (very confident)
   Helps users trust responses

4. SOURCE TRACKING
   Lists which documents contributed to answer
   Enables verification and transparency
   Shows relevance score per source

5. GRACEFUL FALLBACK
   Works without LLM (returns formatted chunks)
   Handles API failures silently
   Always returns something useful

ðŸ’» USAGE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Simple (Auto-detect intent):
    from core.response import generate_response
    
    result = generate_response("How to calculate CGPA?")
    print(result['answer'])
    print(f"Confidence: {result['confidence']}")

RAG-specific (Policy questions):
    from core.response import generate_rag_response
    
    result = generate_rag_response("CGPA calculation rules")
    print(result['answer'])
    for source in result['sources']:
        print(f"- {source['filename']} ({source['relevance']})")

With explicit intent:
    result = generate_response("Roy canteen phone", intent="db_contact")
    print(result['answer'])

Using class directly:
    from core.response import ResponseGenerator
    
    gen = ResponseGenerator()
    result = gen.generate_response("hostel rules")

ðŸ“ RESPONSE FORMAT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

All response methods return a dictionary:

{
    "answer": str,           # Main response text
    "sources": List[Dict],   # Source documents
    "confidence": float,     # 0.0-1.0 confidence score
    "method": str           # How answer was generated
}

Methods:
  â€¢ "rag_hf_llm": RAG + Mistral-7B (best)
  â€¢ "rag_basic": RAG without LLM (fallback)
  â€¢ "rag_no_results": RAG found nothing
  â€¢ "db_contact": Database contact lookup
  â€¢ "db_location": Database location lookup
  â€¢ "ai_fallback": Out-of-scope response

âš ï¸ IMPORTANT NOTES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Requires HUGGINGFACEHUB_ACCESS_TOKEN in .env for LLM
â€¢ Works without LLM but responses less natural
â€¢ Context limited to 2000 chars (prevents token overflow)
â€¢ Query refinement only for queries >3 words (efficiency)
â€¢ Singleton pattern avoids reinitializing LLM
â€¢ All methods return dict format for consistency
"""

# =======================================================================
# IMPORTS
# =======================================================================





# =======================================================================
# LOAD ENV VARIABLES
# =======================================================================




# =======================================================================
# LOGGING SETUP
# =======================================================================





# =======================================================================
# RESPONSE GENERATOR CLASS
# =======================================================================



# ----------------------------------------------------------------------
# GLOBAL HELPERS
# ----------------------------------------------------------------------

